{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "0.6.0.dev\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image as pdImage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json, os, shutil, itertools, fnmatch, re\n",
    "from scipy.interpolate import griddata, make_interp_spline, BSpline\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import cirq\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pickle as pkl\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from collections import defaultdict\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "if sys.platform.startswith('win'):\n",
    "    code_path = \"C:\\\\Users\\\\Andrew Patterson\\\\Google Drive\\\\PhD\\\\First Year\\\\Untitled Folder\\\\cirq_state_discrimination\"\n",
    "else:\n",
    "    code_path = \"/home/zcapga1/Scratch/state_discrimination/code/\"\n",
    "sys.path.append(code_path)\n",
    "print(tf.__version__)\n",
    "print(cirq.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' --output_loc=\"C:\\\\Users\\\\Andrew Patterson\\\\Desktop\"  --create_outputs --restore_loc=\"C:\\\\Users\\\\Andrew Patterson\\\\Documents\\\\PhD\\\\cirq_state_discrimination\\\\checkpoints\\\\myriad_data\\\\new_analysis\\\\tf_noise_array\\\\2019_08_01_21_50_31\"  --use_tf --max_epoch=25 --batch_size=100 --prop_a=0.3333333333333333 --b_const=True --a_const=False --mu_a=0.5 --mu_b=0.75 --sigma_a=0.15 --sigma_b=0.125 --dicts=\"C:\\\\Users\\\\Andrew Patterson\\\\Documents\\\\PhD\\\\cirq_state_discrimination\\\\checkpoints\\\\myriad_data\\\\new_analysis\\\\tf_noise_array\\\\2019_08_01_21_50_31\\\\temp_dicts.pkl\" --cost_error=40.0 --cost_incon=40.0 --no_qubits=4 --noise_on=True --noise_prob=0.0005 --learning_rate=0.001 --beta1=0.9 --beta2=0.999'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_args(path: str) -> str:\n",
    "    args = \" --create_outputs --restore_loc=\\\"{}\\\" \".format(path)\n",
    "    \n",
    "    if re.search(r'tf', path):\n",
    "        args = args + \" --use_tf\"\n",
    "            \n",
    "    with open(os.path.join(path, 'saved_params.json')) as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "#     if params['mu_a'] == 0.5:\n",
    "#         args = args + ' --file_loc=sigma005_mu05_bconst.json'\n",
    "#     elif params['mu_a'] == 0.25:\n",
    "#         args = args + ' --file_loc=sigma005_mu025_bconst.json'\n",
    "#     else:\n",
    "    args = args + ' --max_epoch=25 --batch_size=100'\n",
    "    data_args = ['prop_a', 'b_const', 'a_const', 'mu_a', 'mu_b', 'sigma_a', 'sigma_b']\n",
    "    for a in data_args:\n",
    "        args = args + ' --{}={}'.format(a, str(params[a]))\n",
    "            \n",
    "    dicts = (params['gate_dict'], params['gate_dict_0'], params['gate_dict_1'])\n",
    "    dict_path = os.path.join(path, 'temp_dicts.pkl')\n",
    "    with open(dict_path, 'wb') as f:\n",
    "        pkl.dump(dicts, f, pkl.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    args = args + \" --dicts=\\\"{}\\\"\".format(dict_path)\n",
    "\n",
    "    other_args = ['cost_error', 'cost_incon', 'no_qubits', 'noise_on', \n",
    "                  'noise_prob', 'learning_rate', 'beta1', 'beta2']\n",
    "    for a in other_args:\n",
    "        args = args + ' --{}={}'.format(a, str(params[a]))\n",
    "    return args\n",
    "            \n",
    "def generate_output_file(directory: str) -> None:\n",
    "    folder_list = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "    for restore_path in folder_list:\n",
    "        full_path = os.path.join(directory, restore_path)\n",
    "        if os.path.exists(os.path.join(full_path, 'saved_params.json')):\n",
    "    \n",
    "            args = create_args(full_path)\n",
    "\n",
    "            run_file = os.path.join(code_path, 'run_training.py')\n",
    "            os.system(\"python \\\"\" + run_file +\"\\\"\" + args)\n",
    "        \n",
    "\n",
    "noise_dir = r\"C:\\Users\\Andrew Patterson\\Desktop\"\n",
    "path = \"C:\\\\Users\\\\Andrew Patterson\\\\Documents\\\\PhD\\\\cirq_state_discrimination\\\\checkpoints\\\\myriad_data\\\\new_analysis\\\\tf_noise_array\\\\2019_08_01_21_50_31\"\n",
    "if re.search(\"tf\", path):\n",
    "    print('yes')\n",
    "args = ' --output_loc=\\\"{}\\\" '.format(noise_dir) + create_args(r\"C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data\\new_analysis\\tf_noise_array\\2019_08_01_21_50_31\")\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data\"\n",
    "folders  = os.listdir(path)\n",
    "folders.remove('tf_min_error')\n",
    "folders.remove('tf_min_error_mu025')\n",
    "folders.remove('BAD_RUNS')\n",
    "folders = [os.path.join(path, f) for f in folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_plot(directory: str) -> None:\n",
    "    folder_list = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "    for path in folder_list:\n",
    "        param_path = os.path.join(directory, path, 'saved_params.json')\n",
    "        if os.path.exists(param_path):\n",
    "            plot_path = os.path.join(directory, path, 'outputs')\n",
    "            with open(param_path) as f:\n",
    "                param_dict = json.load(f)\n",
    "            \n",
    "            probs = np.load(os.path.join(plot_path, 'probs.npy'))\n",
    "            p_suc = np.average([probs[0][0], probs[1][1]])\n",
    "            msg = 'Success: {:.2f}, '.format(p_suc)\n",
    "\n",
    "            param_list = ['cost_error', 'cost_incon', 'noise_on', 'noise_prob', 'mu_a', 'sigma_a', 'job_name']\n",
    "            if not param_dict['b_const']:\n",
    "                param_list.extend(['mu_b', 'sigma_b'])\n",
    "\n",
    "            if param_dict['noise_on'] == 'False':\n",
    "                param_list.remove('noise_prob')\n",
    "                \n",
    "            for i, param in enumerate(param_list):\n",
    "                msg = msg + '{}: {}, '.format(param, param_dict[param])\n",
    "                if (i + 1) % 3 == 0:\n",
    "                    msg = msg + \"\\n\"\n",
    "            msg = msg[:-2]\n",
    "            \n",
    "\n",
    "            font_path = os.path.join(\"C:\\\\Users\\\\Andrew Patterson\\\\Documents\\\\PhD\\\\fonts\\\\fonts\\\\ofl\\\\sourcecodepro\", 'SourceCodePro-Light.ttf')\n",
    "            font = ImageFont.truetype(font_path, size=10)\n",
    "            plot = Image.open(os.path.join(plot_path, 'bar_graph.png'))\n",
    "\n",
    "            width, height = plot.size\n",
    "            cropped = plot.crop((100, 35, 570, 435))\n",
    "            draw = ImageDraw.Draw(cropped)\n",
    "            (x,y) = (5,5)\n",
    "            colour = 'rgb(0, 0, 0)'\n",
    "            draw.text((x,y), msg, fill=colour, font=font)\n",
    "            save_path = os.path.join(plot_path, 'plot_labeled.png')\n",
    "            cropped.save(save_path)\n",
    "            os.system(\"convert \\\"{}\\\" -fuzz 2% -transparent white \\\"{}\\\"\".format(save_path, save_path))\n",
    "        \n",
    "# folders = ['tf_noise_off', 'tf_old_dicts_noise_array', 'tf_old_dicts_noise_array', 'tf_old_dicts_noise_array', \n",
    "#            'tf_noise_array', 'tf_noise_array', 'cirq_training_noise_on_array', 'tf_noise_off_long', \n",
    "#            'tf_old_dicts_noise_array_long', 'tf_old_dicts_noise_off_long', 'cirq_training_noise_on_array_long', \n",
    "#           'tf_noise_array_long', 'tf_min_error', 'tf_min_error_mu025', 'tf_short_dicts_noise_array', \n",
    "#            'tf_min_error_mu025_2_60_sigma_001', 'tf_old_2_60_sigma001', 'tf_old_2_60_sigma001_mu025', \n",
    "#            'tf_min_error_2_60', 'tf_noise_array_mu025']\n",
    "# folders = ['tf_noise_array']\n",
    "# direc = r'C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data'\n",
    "# # folders = os.listdir(direc)\n",
    "# for f in folders:\n",
    "#     path = os.path.join(direc, f)\n",
    "#     label_plot(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_plot_val_levels(directory: str) -> None:\n",
    "    folder_list = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "    for path in folder_list:\n",
    "        param_path = os.path.join(directory, path, 'saved_params.json')\n",
    "        if os.path.exists(param_path):\n",
    "            \n",
    "            with open(param_path) as f:\n",
    "                param_dict = json.load(f)\n",
    "\n",
    "            param_list = ['cost_error', 'cost_incon', 'noise_on', 'noise_prob', 'mu_a', 'sigma_a', 'job_name']\n",
    "            if not param_dict['b_const']:\n",
    "                param_list.extend(['mu_b', 'sigma_b'])\n",
    "            noise_levels = os.path.join(directory, path, 'outputs_with_noise_levels')\n",
    "            levels = [f for f in os.listdir(noise_levels) if os.path.isdir(os.path.join(noise_levels, f))]\n",
    "            for level in levels:\n",
    "                probs = np.load(os.path.join(noise_levels, level, 'probs.npy'))\n",
    "                p_suc = np.average([probs[0][0], probs[1][1]])\n",
    "                msg = 'Success: {:.2f}, '.format(p_suc)\n",
    "                for i, param in enumerate(param_list):\n",
    "                    msg = msg + '{}: {}, '.format(param, param_dict[param])\n",
    "                    if (i + 1) % 3 == 0:\n",
    "                        msg = msg + \"\\n\"\n",
    "                msg = msg.replace('noise_prob', 'training_noise_prob')\n",
    "                msg = msg + 'validation_noise_prob : {} '.format(level.replace('_', '.'))\n",
    "\n",
    "                font_path = os.path.join(\"C:\\\\Users\\\\Andrew Patterson\\\\Documents\\\\PhD\\\\fonts\\\\fonts\\\\ofl\\\\sourcecodepro\", 'SourceCodePro-Light.ttf')\n",
    "                font = ImageFont.truetype(font_path, size=10)\n",
    "                plot = Image.open(os.path.join(noise_levels, level, 'bar_graph.png'))\n",
    "\n",
    "                width, height = plot.size\n",
    "                cropped = plot.crop((100, 35, 570, 435))\n",
    "                draw = ImageDraw.Draw(cropped)\n",
    "                (x,y) = (5,5)\n",
    "                colour = 'rgb(0, 0, 0)'\n",
    "                draw.text((x,y), msg, fill=colour, font=font)\n",
    "                save_path = os.path.join(noise_levels, level, 'plot_labeled.png')\n",
    "                cropped.save(save_path)\n",
    "                os.system(\"convert \\\"{}\\\" -fuzz 1% -transparent white \\\"{}\\\"\".format(save_path, save_path))\n",
    "\n",
    "# folders = ['tf_old_2_60_sigma001', 'tf_old_2_60_sigma001_mu025', 'tf_min_error_2_60', 'tf_noise_array_mu025']\n",
    "# direc = r'C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data'\n",
    "# for f in folders:\n",
    "#     path = os.path.join(direc, f)\n",
    "#     label_plot_val_levels(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nwise(iterable, n):\n",
    "    ts = itertools.tee(iterable, n)\n",
    "    for c, t in enumerate(ts):\n",
    "        next(itertools.islice(t, c, c), None)\n",
    "    return zip(*ts)\n",
    "\n",
    "def moving_average(iterable, n):\n",
    "    yield from (sum(x)/n for x in nwise(iterable, n))\n",
    "    \n",
    "def plot_loss_fn(folder, n=6, cutoff=100000):\n",
    "    \"\"\"\n",
    "    Input: Takes a list of folders where models are strored and returns a graph of convergences plotted together \n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training loss, Moving average = {}'.format(n))\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    event_file = fnmatch.filter(files, 'events*')\n",
    "    steps = []\n",
    "    loss = []\n",
    "    for e in tf.compat.v1.train.summary_iterator(os.path.join(folder, event_file[0])):\n",
    "        for v in e.summary.value:\n",
    "            if v.tag == 'Training loss':\n",
    "                if e.step <= cutoff:\n",
    "                    steps.append(e.step)\n",
    "                    val = np.frombuffer(v.tensor.tensor_content, dtype=np.float32)[0]\n",
    "                    loss.append(val)\n",
    "\n",
    "    param_file = os.path.join(folder, 'saved_params.json')\n",
    "    with open(param_file, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    if params['noise_on'] == \"False\":\n",
    "        noise_level = 0\n",
    "    else:\n",
    "        noise_level = params['noise_prob']\n",
    "    tot_cost = float(params['cost_error']) + float(params['cost_incon'])\n",
    "    loss = np.array(loss)\n",
    "    loss = loss / tot_cost\n",
    "    np.save((step, loss), os.path.join(folder, 'outputs', 'loss_fn.npy'))\n",
    "    loss_ma = moving_average(loss, n)\n",
    "    step_ma = moving_average(steps, n)\n",
    "    ax.plot(list(step_ma), list(loss_ma), label=noise_level)\n",
    "    ax.legend()\n",
    "    plt.savefig(os.path.join(folder, 'outputs', 'loss_fn.png'))\n",
    "    plt.show()\n",
    "\n",
    "def save_loss_fns(directory: str):\n",
    "    folder_list = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "    for path in folder_list:\n",
    "        plot_loss_fn(os.path.join(directory, path))\n",
    "# save_loss_fns(r\"C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data\\tf_noise_off_g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_frame(directory: str):\n",
    "    df = pd.DataFrame()\n",
    "    folder_list = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "    for f in folder_list:\n",
    "        path = os.path.join(directory, f)\n",
    "        probs_path = os.path.join(path, 'outputs', 'probs.npy')\n",
    "        if os.path.isfile(probs_path):\n",
    "            with open(os.path.join(path, 'saved_params.json')) as js:\n",
    "                params = json.load(js)\n",
    "            probs = np.load(probs_path)\n",
    "\n",
    "            p_error = np.average([probs[0][1], probs[1][0]])\n",
    "            p_inc = np.average([probs[0][2], probs[1][2]])\n",
    "            p_suc = np.average([probs[0][0], probs[1][1]])\n",
    "\n",
    "            loss = p_error + p_inc\n",
    "\n",
    "            df = df.append({'P_err': p_error, 'P_inc': p_inc, 'P_suc': p_suc, 'cost_err': params['cost_error'],\n",
    "                            'cost_inc': params['cost_incon'], 'noise_on': params['noise_on'],\n",
    "                            'noise_prob': params['noise_prob'], 'learning_rate': params['learning_rate'],\n",
    "                            'loss': loss, 'folder': str(f)},\n",
    "                           ignore_index=True)\n",
    "\n",
    "\n",
    "    numeric = ['cost_err', 'cost_inc', 'noise_prob', 'learning_rate']\n",
    "    df[numeric] = df[numeric].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "    if not os.path.exists(os.path.join(directory, 'output')):\n",
    "        os.mkdir(os.path.join(directory, 'output'))\n",
    "    df.to_pickle(os.path.join(directory, 'output', 'dataframe.pkl'))\n",
    "    f, ax = plt.subplots(figsize=(7,7))\n",
    "    ax.set(xscale='log', title='Loss with log noise')\n",
    "    df.plot(x='noise_prob', y='loss', ax=ax, style='k--')\n",
    "    plt.savefig(os.path.join(directory, 'output', 'loss_noise.png'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = create_data_frame(r\"C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data\\tf_noise_array_g\")\n",
    "# f, ax = plt.subplots(figsize=(7,7))\n",
    "# ax.set(xscale='log', title='Loss with log noise')\n",
    "# df.plot(x='noise_prob', y='loss', ax=ax, style='k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_trained_new_noise_level(restore_directory: str, use_tf: bool, noise_levels: list):\n",
    "    noise_levels_path = os.path.join(restore_directory, 'outputs_with_noise_levels')\n",
    "    if not os.path.exists(noise_levels_path):\n",
    "        os.mkdir(noise_levels_path)\n",
    "        \n",
    "    for noise in noise_levels:\n",
    "        args_str = create_args(restore_directory)\n",
    "        args_str = re.sub('(--noise_prob=\\d*.\\d*)', \"--noise_prob={}\".format(noise), args_str)\n",
    "        args_str = re.sub('(--noise_on=\\w*|\\\"\\w*\\\")', \"--noise_on=True\", args_str)\n",
    "        \n",
    "        noise_dir = os.path.join(noise_levels_path, re.sub('\\.', '_', str(noise)))\n",
    "        args_str = args_str + ' --output_loc=\\\"{}\\\"'.format(noise_dir)\n",
    "        run_file = os.path.join(code_path, 'run_training.py')\n",
    "        os.system(\"python \\\"\" + run_file +\"\\\"\" + args_str)\n",
    "        \n",
    "def pre_trained_multiple(directory: str, noise_levels: list):\n",
    "    folder_list = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "    for path in folder_list:\n",
    "        # if (not os.path.exists(os.path.join(directory, path, 'outputs_with_noise_levels'))) and os.path.exists(os.path.join(directory, path, 'saved_params.json')):\n",
    "        if os.path.exists(os.path.join(directory, path, 'saved_params.json')):\n",
    "            if re.match(r'tf_', path):\n",
    "                use_tf = True\n",
    "            else:\n",
    "                use_tf = False\n",
    "            pre_trained_new_noise_level(os.path.join(directory, path), use_tf, noise_levels)\n",
    "\n",
    "noise_levels = [0.0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25]        \n",
    "# pre_trained_multiple(r\"C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data\\tf_noise_off_g\", True, noise_levels)\n",
    "# pre_trained_multiple(r\"C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data\\tf_noise_array\", noise_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep=[\"#4C72B0\", \"#55A868\", \"#C44E52\",\n",
    "      \"#8172B2\", \"#CCB974\", \"#64B5CD\"]\n",
    "muted=[\"#4878CF\", \"#6ACC65\", \"#D65F5F\",\n",
    "       \"#B47CC7\", \"#C4AD66\", \"#77BEDB\"]\n",
    "pastel=[\"#92C6FF\", \"#97F0AA\", \"#FF9F9A\",\n",
    "        \"#D0BBFF\", \"#FFFEA3\", \"#B0E0E6\"]\n",
    "bright=[\"#003FFF\", \"#03ED3A\", \"#E8000B\",\n",
    "        \"#8A2BE2\", \"#FFC400\", \"#00D7FF\"]\n",
    "dark=[\"#001C7F\", \"#017517\", \"#8C0900\",\n",
    "      \"#7600A1\", \"#B8860B\", \"#006374\"]\n",
    "colorblind=[\"#0072B2\", \"#009E73\", \"#D55E00\",\n",
    "            \"#CC79A7\", \"#F0E442\", \"#56B4E9\"]\n",
    "pal = dark\n",
    "noise_pal = {0.0: pal[0], 0.0005: pal[1], 0.001: pal[2], 0.005: pal[3],  0.01: pal[4], 0.05: pal[5], 0.1: pal[0], 0.25:pal[1]}\n",
    "\n",
    "def create_noise_levels_df(directory: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    params_path = os.path.join(directory, 'saved_params.json')\n",
    "    with open(params_path) as js:\n",
    "        params = json.load(js)\n",
    "    noise_folder = os.path.join(directory, 'outputs_with_noise_levels')\n",
    "    noise_paths = [f for f in os.listdir(noise_folder) if os.path.isdir(os.path.join(noise_folder, f))]\n",
    "    for p in noise_paths:\n",
    "        path = os.path.join(noise_folder, p)\n",
    "        probs = np.load(os.path.join(path, 'probs.npy'))\n",
    "        \n",
    "        p_error = np.average([probs[0][1], probs[1][0]])\n",
    "        p_inc = np.average([probs[0][2], probs[1][2]])\n",
    "        p_suc = np.average([probs[0][0], probs[1][1]])\n",
    "\n",
    "        loss = p_error + p_inc\n",
    "        \n",
    "        if params['noise_on'] == 'True':\n",
    "            training_noise = params['noise_prob']\n",
    "        else:\n",
    "            training_noise = 0\n",
    "\n",
    "        df = df.append({'P_err': p_error, 'P_inc': p_inc, 'cost_err': params['cost_error'],\n",
    "                        'cost_inc': params['cost_incon'],'training_noise': training_noise, \n",
    "                        'noise_prob': re.sub('\\_', '.', p), 'loss': loss},\n",
    "                       ignore_index=True)\n",
    "    return df\n",
    "        \n",
    "def log_on_one_plot(directory: str):\n",
    "    df = pd.DataFrame()\n",
    "    folder_list = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "    for f in folder_list:\n",
    "        if os.path.exists(os.path.join(directory, f, 'saved_params.json')):\n",
    "            df = df.append(create_noise_levels_df(os.path.join(directory, f)))\n",
    "    f, ax = plt.subplots(figsize=(7,7))\n",
    "    #ax.set(xscale='log', title='Loss with log noise')\n",
    "    \n",
    "    numeric = ['cost_err', 'cost_inc', 'noise_prob', 'loss']\n",
    "    df[numeric] = df[numeric].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "    for x in [0.0005, 0.005, 0.25]:\n",
    "        df = df[df.training_noise != x]\n",
    "    df['training_noise'] = df['training_noise'].astype('category')\n",
    "    sns.scatterplot(x='noise_prob', y='loss', style='training_noise', data=df, hue='training_noise', palette=noise_pal)\n",
    "#     for key, group in df.groupby('training_noise'):\n",
    "#         sns.scatterplot(group['noise_prob'], group['loss'], legend='full')\n",
    "    if not os.path.exists(os.path.join(directory, 'output')):\n",
    "        os.mkdir(os.path.join(directory, 'output'))\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    ax.set(xscale='log', yscale='log', title='Log loss vs. log validation noise level\\nfor different levels of training noise.')\n",
    "    ax.set(xlim=(0.00045, 0.3), ylim=(0.002,1))\n",
    "    plt.savefig(os.path.join(directory, 'output', 'log_noise_plot.png'))\n",
    "    df.to_pickle(os.path.join(directory, 'output', 'noise_df.pkl'))\n",
    "\n",
    "# log_on_one_plot(r'C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data\\tf_noise_off_g')    \n",
    "# log_on_one_plot(r'C:\\Users\\Andrew Patterson\\Documents\\PhD\\cirq_state_discrimination\\checkpoints\\myriad_data\\tf_noise_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform.startswith('win'):\n",
    "    checkpoints = \"C:\\\\Users\\\\Andrew Patterson\\\\Documents\\\\PhD\\\\cirq_state_discrimination\\\\checkpoints\\\\myriad_data\\\\\"\n",
    "else:\n",
    "    checkpoints = \"/home/zcapga1/Scratch/state_discrimination/training_out/\"\n",
    "\n",
    "# runs = ['tf_noise_off', 'tf_old_dicts_noise_off', 'cirq_training_noise_off', 'tf_noise_array', 'tf_old_dicts_noise_array', 'tf_old_dicts_noise_off']\n",
    "runs = os.listdir(checkpoints)\n",
    "rm = ['BAD_RUNS', 'cirq_training_noise_off', 'cirq_training_noise_on_array', 'cirq_training_noise_off_long', 'cirq_training_noise_on_array_long']\n",
    "for r in rm:\n",
    "    runs.remove(r)\n",
    "folder_list = [os.path.join(checkpoints, f) for f in runs]\n",
    "\n",
    "noise_levels = [0.0, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25]\n",
    "def run_on_all_folders(folders: list, noise_levels: list):\n",
    "    for folder in folders:\n",
    "        generate_output_file(folder)\n",
    "        label_plot(folder)\n",
    "        save_loss_fns(folder)\n",
    "        create_data_frame(folder)\n",
    "        pre_trained_multiple(folder, noise_levels)\n",
    "        label_plot_val_levels(folder)\n",
    "        log_on_one_plot(folder)\n",
    "        \n",
    "# run_on_all_folders(folder_list, noise_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['new_analysis\\\\tf_noise_array tr:0.0005, vl:0', 'new_analysis\\\\tf_noise_array tr:0.005, vl:0', 'new_analysis\\\\tf_noise_array tr:0.005, vl:0.05', 'new_analysis\\\\tf_noise_array tr:0.005, vl:0.1', 'new_analysis\\\\tf_noise_array tr:0.001, vl:0.05', 'new_analysis\\\\tf_noise_array tr:0.001, vl:0.1', 'new_analysis\\\\tf_noise_array tr:0.01, vl:0', 'new_analysis\\\\tf_noise_array tr:0.05, vl:0', 'new_analysis\\\\tf_noise_array tr:0.1, vl:0', 'new_analysis\\\\tf_noise_array tr:0.1, vl:0.01', 'new_analysis\\\\tf_noise_array tr:0.1, vl:0.05', 'new_analysis\\\\tf_noise_array tr:0.25, vl:0.001', 'new_analysis\\\\tf_noise_array tr:0.25, vl:0.01', 'new_analysis\\\\tf_noise_array tr:0.25, vl:0.05', 'new_analysis\\\\tf_noise_array tr:0.25, vl:0.1'], []]\n"
     ]
    }
   ],
   "source": [
    "def check_probs(directory: str):\n",
    "    flagged_folders = []\n",
    "    okay = []\n",
    "    folder_list = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "    for folder in folder_list:\n",
    "        full_path = os.path.join(directory, folder)\n",
    "        if os.path.exists(os.path.join(full_path, 'saved_params.json')):\n",
    "            with open(os.path.join(full_path, 'saved_params.json'), 'r') as f:\n",
    "                params = json.load(f)\n",
    "            tr_noise = params['noise_prob']\n",
    "            probs = np.load(os.path.join(full_path, 'outputs', 'probs.npy'))\n",
    "            if (not np.isclose(np.sum(probs[0]), 1, atol=0.0001, rtol=0.1)) or (not np.isclose(np.sum(probs[1]), 1, atol=0.0001, rtol=0.1)):\n",
    "                flagged_folders.append(directory[90:] + ' tr:{}'.format(tr_noise))\n",
    "                \n",
    "            noise_outputs = os.path.join(full_path, 'outputs_with_noise_levels')\n",
    "            noise_levels = [f for f in os.listdir(noise_outputs) if os.path.isdir(os.path.join(noise_outputs, f))]\n",
    "            for noise in noise_levels:\n",
    "                noise_path = os.path.join(noise_outputs, noise)\n",
    "                probs = np.load(os.path.join(noise_path, 'probs.npy'))\n",
    "                if (not np.isclose(np.sum(probs[0]), 1, atol=0.0001, rtol=0.1)) or (not np.isclose(np.sum(probs[1]), 1, atol=0.0001, rtol=0.1)):\n",
    "                    flagged_folders.append(directory[90:] + ' tr:{}, vl:{}'.format(tr_noise, noise.replace('_', '.')))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    return flagged_folders, okay\n",
    "\n",
    "flagged_out = []\n",
    "okay_out = []\n",
    "direc = \"C:\\\\Users\\\\Andrew Patterson\\\\Documents\\\\PhD\\\\cirq_state_discrimination\\\\checkpoints\\\\myriad_data\\\\new_analysis\"\n",
    "folder_list = [os.path.join(direc, f) for f in os.listdir(direc) if os.path.isdir(os.path.join(direc, f))]\n",
    "\n",
    "for f in folder_list:\n",
    "    flagged, okay = check_probs(f)\n",
    "    flagged_out.append(flagged)\n",
    "    okay_out.append(okay)\n",
    "print(flagged_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"C:\\\\Users\\\\Andrew Patterson\\\\Documents\\\\PhD\\\\cirq_state_discrimination\\\\checkpoints\\\\myriad_data\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirq_dev",
   "language": "python",
   "name": "cirq_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
